[project]
name = "llama3.1-8b-kv-offloading"
version = "1.4.25"
description = "Self-host LLMs with vLLM and BentoML"
readme = "README.md"
requires-python = ">=3.11"
authors = [
  { name = "BentoML Team", email = "contact@bentoml.com" },
]

[project.license]
text = "Apache-2.0"

[project.urls]
Website = "https://bentoml.com"
Documentation = "https://docs.bentoml.com"
GitHub = "https://github.com/bentoml/OpenLLM"
Twitter = "https://twitter.com/bentomlai"
Tracker = "https://github.com/bentoml/OpenLLM/issues"

[tool.bentoml.build]
service = "service.py:LLM"
include = [
  "*.py",
  "*.toml",
  "*.txt",
  "*.md",
  "templates/*",
]

[tool.bentoml.build.args]
name = "llama3.1-8b-kv-offloading"
gpu_type = "nvidia-h100-80gb"
tp = 1
autotune = [
  1,
  2,
  4,
  8,
  16,
]
cli_args = [
  "--max-num-batched-tokens",
  "16384",
]
max_model_len = 16384
model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"
tool_parser = "llama3_json"
envs = [
  { name = "HF_TOKEN" },
  { name = "LMCACHE_CHUNK_SIZE", value = "256" },
  { name = "LMCACHE_LOCAL_CPU", value = "True" },
  { name = "LMCACHE_MAX_LOCAL_CPU_SIZE", value = "5.0" },
]

[tool.bentoml.build.args.metadata]
description = "Llama 3.1 8B Instruct with CPU KV offloading"
provider = "Meta"
gpu_recommendation = "an Nvidia GPU with at least 40GB VRAM (e.g about 1 A100 GPU)."

[tool.bentoml.build.args.kv_transfer_config]
kv_connector = "LMCacheConnectorV1"
kv_role = "kv_both"

[tool.bentoml.build.args.hf_generation_config]
temperature = 0.6
top_p = 0.9

[tool.ruff]
extend-include = [
  "*.ipynb",
]
preview = true
line-length = 119
indent-width = 2

[tool.ruff.format]
preview = true
quote-style = "single"
indent-style = "space"
skip-magic-trailing-comma = true
docstring-code-format = true

[tool.ruff.lint]
ignore = [
  "RUF012",
  "ANN",
  "E722",
]
select = [
  "F",
  "G",
  "PERF",
  "RUF",
  "W6",
  "E71",
  "E72",
  "E112",
  "E113",
  "E203",
  "E272",
  "E502",
  "E702",
  "E703",
  "E731",
  "W191",
  "W291",
  "W293",
  "UP039",
]

[tool.ruff.lint.pydocstyle]
convention = "google"
