args:
  name: llama3.1-8b-kv-offloading
  gpu_type: nvidia-h100-80gb
  tp: 1
  autotune: [1,2,4,8,16]
  cli_args:
    - '--max-num-batched-tokens'
    - '16384'
  max_model_len: 16384
  metadata:
    description: Llama 3.1 8B Instruct with CPU KV offloading
    provider: Meta
    gpu_recommendation: an Nvidia GPU with at least 40GB VRAM (e.g about 1 A100 GPU).
  model_id: meta-llama/Meta-Llama-3.1-8B-Instruct
  tool_parser: llama3_json
  envs:
    - name: HF_TOKEN
    - name: LMCACHE_CHUNK_SIZE
      value: "256"
    - name: LMCACHE_LOCAL_CPU
      value: "True"
    - name: LMCACHE_MAX_LOCAL_CPU_SIZE
      value: "5.0"
  kv_transfer_config:
    kv_connector: "LMCacheConnectorV1"
    kv_role: "kv_both"
  hf_generation_config:
    temperature: 0.6
    top_p: 0.9
