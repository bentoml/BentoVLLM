args:
  name: llama3.1-8b-instruct
  tp: 1
  autotune: [1,2,4,8,16]
  cli_args:
    - '--max-num-batched-tokens'
    - '16384'
  metadata:
    description: Llama 3.1 8B Instruct
    provider: Meta
    gpu_recommendation: an Nvidia GPU with at least 40GB VRAM (e.g about 1 A100 GPU).
  model_id: meta-llama/Meta-Llama-3.1-8B-Instruct
  tool_parser: llama3_json
  envs:
    - name: HF_TOKEN
  hf_generation_config:
    temperature: 0.6
    top_p: 0.9
