"deepseek-v3-671b":
  use_mla: true
  metadata:
    description: DeepSeek V3 671B
    provider: DeepSeek
    gpu_recommendation: Nvidia GPUs with at least 80GBx16 VRAM (e.g. about 8 H200 GPUs).
  service_config:
    name: bentovllm-deepseek-v3-671b-service
    traffic:
      timeout: 300
    resources:
      gpu: 8
      gpu_type: nvidia-h200-141gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-V3
    max_model_len: 8192
    tensor_parallel_size: 8
    max_num_seqs: 1024
  generate_config:
    max_tokens: 4096
"deepseek-r1-671b":
  reasoning: true
  use_mla: true
  metadata:
    description: DeepSeek R1 671B
    provider: DeepSeek
    gpu_recommendation: Nvidia GPUs with at least 80GBx16 VRAM (e.g. about 8 H200 GPUs).
  service_config:
    name: bentovllm-deepseek-r1-671b-service
    traffic:
      timeout: 300
    resources:
      gpu: 8
      gpu_type: nvidia-h200-141gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-R1
    tensor_parallel_size: 8
    trust_remote_code: true
    max_model_len: 8192
    enable_reasoning: true
    reasoning_parser: "deepseek_r1"
    max_num_seqs: 1024
  generate_config:
    max_tokens: 4096
"deepseek-r1-distill-llama3.3-70b":
  reasoning: true
  metadata:
    description: DeepSeek R1 Distill Llama 3.3 70B
    provider: DeepSeek
    gpu_recommendation: Nvidia GPUs with at least 80GBx2 VRAM (e.g about 2 H100 GPUs).
  service_config:
    name: bentovllm-r1-llama3.3-70b-service
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-R1-Distill-Llama-70B
    tensor_parallel_size: 2
    max_model_len: 8192
    enable_reasoning: true
    reasoning_parser: "deepseek_r1"
    max_num_seqs: 1024
    enable_auto_tool_choice: true
    tool_call_parser: "llama3_json"
  generate_config:
    max_tokens: 4096
"deepseek-r1-distill-qwen2.5-32b":
  reasoning: true
  metadata:
    description: DeepSeek R1 Distill Qwen 2.5 32B
    provider: DeepSeek
    gpu_recommendation: an Nvidia GPU with at least 80GB VRAM (e.g about 1 H100 GPU).
  service_config:
    name: bentovllm-r1-qwen2.5-32b-service
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    traffic:
      timeout: 300
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
    max_model_len: 8192
    enable_reasoning: true
    reasoning_parser: "deepseek_r1"
    max_num_seqs: 1024
    enable_auto_tool_choice: true
    tool_call_parser: "hermes"
  generate_config:
    max_tokens: 4096
"deepseek-r1-distill-qwen2.5-7b-math":
  reasoning: true
  metadata:
    description: DeepSeek R1 Distill Qwen 2.5 Math 7B
    provider: DeepSeek
    gpu_recommendation: an Nvidia GPU with at least 24GB VRAM (e.g about 1 L4 GPU).
  service_config:
    name: bentovllm-r1-qwen2.5-7b-math-service
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
    max_model_len: 8192
    enable_reasoning: true
    reasoning_parser: "deepseek_r1"
    enable_auto_tool_choice: true
    tool_call_parser: "hermes"
  generate_config:
    max_tokens: 4096
"deepseek-r1-distill-llama3.1-8b-tool-calling":
  reasoning: true
  metadata:
    description: DeepSeek R1 Distill Llama 3.1 8B Tool Calling
    provider: DeepSeek
    gpu_recommendation: an Nvidia GPU with at least 24GB VRAM (e.g about 1 L4 GPU).
  service_config:
    name: bentovllm-r1-llama3.1-8b-tool-calling-service
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-R1-Distill-Llama-8B
    max_model_len: 4096
    enable_reasoning: true
    reasoning_parser: "deepseek_r1"
    enable_auto_tool_choice: true
    tool_call_parser: "llama3_json"
  generate_config:
    max_tokens: 2048
"gemma3-4b-instruct":
  vision: true
  metadata:
    description: Gemma 3 4B Instruct
    provider: Google
    gpu_recommendation: an Nvidia GPU with at least 16GB VRAM (e.g about 1 L4 GPU).
  service_config:
    name: bentovllm-gemma3-4b-instruct-service
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: google/gemma-3-4b-it
    max_model_len: 16384
  generate_config:
    max_tokens: 8192
"gemma2-2b-instruct":
  metadata:
    description: Gemma 2 2B Instruct
    provider: Google
    gpu_recommendation: an Nvidia GPU with at least 16GB VRAM (e.g about 1 L4 GPU).
  service_config:
    name: bentovllm-gemma2-2b-instruct-service
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: google/gemma-2-2b-it
    max_model_len: 2048
    dtype: half
    enable_prefix_caching: false
    chat_template: "{% if messages[0]['role'] == 'system' %}\n    {% set loop_messages = messages[1:] %}\n    {% set system_message = messages[0]['content'].strip() + '\\n\\n' %}\n{% else %}\n    {% set loop_messages = messages %}\n    {% set system_message = '' %}\n{% endif %}\n\n{% for message in loop_messages %}\n    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n    {% endif %}\n\n    {% if loop.index0 == 0 %}\n        {% set content = system_message + message['content'] %}\n    {% else %}\n        {% set content = message['content'] %}\n    {% endif %}\n\n    {% if (message['role'] == 'assistant') %}\n        {% set role = 'model' %}\n    {% else %}\n        {% set role = message['role'] %}\n    {% endif %}\n\n    {{ '<start_of_turn>' + role + '\\n' + content.strip() + '<end_of_turn>\\n' }}\n\n    {% if loop.last and message['role'] == 'user' and add_generation_prompt %}\n        {{'<start_of_turn>model\\n'}}\n    {% endif %}\n{% endfor %}\n"
  generate_config:
    max_tokens: 1024
"gemma2-27b-instruct":
  metadata:
    description: Gemma 2 27B Instruct
    provider: Google
    gpu_recommendation: an Nvidia GPU with at least 80GB VRAM (e.g about 1 H100 GPU).
  service_config:
    name: bentovllm-gemma2-27b-instruct-service
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: google/gemma-2-27b-it
    max_model_len: 2048
    dtype: half
    enable_prefix_caching: false
    max_num_seqs: 1024
    chat_template: "{% if messages[0]['role'] == 'system' %}\n    {% set loop_messages = messages[1:] %}\n    {% set system_message = messages[0]['content'].strip() + '\\n\\n' %}\n{% else %}\n    {% set loop_messages = messages %}\n    {% set system_message = '' %}\n{% endif %}\n\n{% for message in loop_messages %}\n    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n    {% endif %}\n\n    {% if loop.index0 == 0 %}\n        {% set content = system_message + message['content'] %}\n    {% else %}\n        {% set content = message['content'] %}\n    {% endif %}\n\n    {% if (message['role'] == 'assistant') %}\n        {% set role = 'model' %}\n    {% else %}\n        {% set role = message['role'] %}\n    {% endif %}\n\n    {{ '<start_of_turn>' + role + '\\n' + content.strip() + '<end_of_turn>\\n' }}\n\n    {% if loop.last and message['role'] == 'user' and add_generation_prompt %}\n        {{'<start_of_turn>model\\n'}}\n    {% endif %}\n{% endfor %}\n"
  generate_config:
    max_tokens: 1024
"jamba1.5-mini":
  metadata:
    description: Jamba 1.5 Mini
    provider: AI21 Lab
    gpu_recommendation: Nvidia GPUs with at least 80GBx2 VRAM (e.g about 2 H100 GPUs).
  service_config:
    name: bentovllm-jamba1.5-mini-service
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
      - name: UV_NO_BUILD_ISOLATION
        value: 1
  engine_config:
    model: ai21labs/AI21-Jamba-1.5-Mini
    max_model_len: 204800
    tensor_parallel_size: 2
    enable_prefix_caching: false
    max_num_seqs: 1024
    enable_auto_tool_choice: true
    tool_call_parser: "jamba"
  build:
    system_packages:
      - curl
      - git
    post:
      - uv pip install --compile-bytecode torch
      - curl -L -o ./causal_conv1d-1.5.0.post8+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.5.0.post8/causal_conv1d-1.5.0.post8+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl
      - uv pip install --compile-bytecode ./causal_conv1d-1.5.0.post8+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl
      - curl -L -o ./mamba_ssm-2.2.4+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl https://github.com/state-spaces/mamba/releases/download/v2.2.4/mamba_ssm-2.2.4+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl
      - uv pip install --compile-bytecode ./mamba_ssm-2.2.4+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl
  generate_config:
    max_tokens: 4096
"jamba1.5-large":
  metadata:
    description: Jamba 1.5 Large
    provider: AI21 Lab
    gpu_recommendation: Nvidia GPUs with at least 80GBx8 VRAM (e.g about 8 H100 GPUs).
  service_config:
    name: bentovllm-jamba1.5-large-service
    traffic:
      timeout: 300
    resources:
      gpu: 8
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
      - name: UV_NO_BUILD_ISOLATION
        value: 1
  engine_config:
    model: ai21labs/AI21-Jamba-1.5-Large
    max_model_len: 225280
    tensor_parallel_size: 8
    quantization: "experts_int8"
    enable_prefix_caching: false
    max_num_seqs: 1024
    enable_auto_tool_choice: true
    tool_call_parser: "jamba"
  build:
    system_packages:
      - curl
      - git
    post:
      - uv pip install --compile-bytecode torch
      - curl -L -o ./causal_conv1d-1.5.0.post8+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.5.0.post8/causal_conv1d-1.5.0.post8+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl
      - uv pip install --compile-bytecode ./causal_conv1d-1.5.0.post8+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl
      - curl -L -o ./mamba_ssm-2.2.4+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl https://github.com/state-spaces/mamba/releases/download/v2.2.4/mamba_ssm-2.2.4+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl
      - uv pip install --compile-bytecode ./mamba_ssm-2.2.4+cu12torch2.5cxx11abiFALSE-cp311-cp311-linux_x86_64.whl
  generate_config:
    max_tokens: 4096
"llama3.1-8b-instruct":
  metadata:
    description: Llama 3.1 8B Instruct
    provider: Meta
    gpu_recommendation: an Nvidia GPU with at least 24GB VRAM (e.g about 1 L4 GPU).
  service_config:
    name: bentovllm-llama3.1-8b-instruct-service
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: meta-llama/Meta-Llama-3.1-8B-Instruct
    max_model_len: 4096
    enable_auto_tool_choice: true
    tool_call_parser: "llama3_json"
  generate_config:
    max_tokens: 2048
"llama3.2-3b-instruct":
  metadata:
    description: Llama 3.1 3B Instruct
    provider: Meta
    gpu_recommendation: an Nvidia GPU with at least 16GB VRAM (e.g about 1 T4 GPU).
  service_config:
    name: bentovllm-llama3.2-3b-instruct-service
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-t4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: meta-llama/Llama-3.2-3B-Instruct
    max_model_len: 8192
    enable_auto_tool_choice: true
    tool_call_parser: "pythonic"
  generate_config:
    max_tokens: 4096
"llama3.2-11b-vision-instruct":
  metadata:
    description: Llama 3.2 11B Vision Instruct
    provider: Meta
    gpu_recommendation: an Nvidia GPU with at least 80GB VRAM (e.g about 1 H100 GPU).
  vision: true
  service_config:
    name: bentovllm-llama3.2-11b-vision-instruct-service
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: meta-llama/Llama-3.2-11B-Vision-Instruct
    enforce_eager: true
    limit_mm_per_prompt:
      image: 1
    max_model_len: 16384
    max_num_seqs: 16
    enable_auto_tool_choice: true
    tool_call_parser: "pythonic"
  build:
    exclude:
      - "original"
  generate_config:
    max_tokens: 8192
"llama4-17b-maverick-instruct":
  vision: true
  c2a: true
  metadata:
    description: Llama 4 Maverick 17B-128E Instruct
    provider: Meta
    gpu_recommendation: Nvidia GPUs with at least 80GBx8 VRAM (e.g about 8 H100 GPUs).
  service_config:
    name: bentovllm-llama4-17b-maverick-instruct-service
    traffic:
      timeout: 300
    resources:
      gpu: 8
      gpu_type: nvidia-tesla-h100
    envs:
      - name: HF_TOKEN
      - name: VLLM_DISABLE_COMPILE_CACHE
        value: "1"
  engine_config:
    model: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
    max_model_len: 430000
    tensor_parallel_size: 8
    max_num_seqs: 256
    override_generation_config:
      attn_temperature_tuning: true
    tool_call_parser: "pythonic"
    enable_auto_tool_choice: true
  build:
    exclude:
      - "original"
      - "consolidated*"
  generate_config:
    max_tokens: 2048
"llama4-17b-scout-instruct":
  vision: true
  c2a: true
  metadata:
    description: Llama 4 Scout 17B-16E Instruct
    provider: Meta
    gpu_recommendation: Nvidia GPUs with at least 80GBx2 VRAM (e.g about 2 H100 GPUs).
  service_config:
    name: bentovllm-llama4-17b-scout-instruct-service
    traffic:
      timeout: 300
    resources:
      gpu: 4
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
      - name: VLLM_DISABLE_COMPILE_CACHE
        value: "1"
  engine_config:
    model: meta-llama/Llama-4-Scout-17B-16E-Instruct
    max_model_len: 16384
    tensor_parallel_size: 4
    max_num_seqs: 256
    override_generation_config:
      attn_temperature_tuning: true
    tool_call_parser: "pythonic"
    enable_auto_tool_choice: true
  build:
    exclude:
      - "original"
      - "consolidated*"
  generate_config:
    max_tokens: 2048
"llama3.3-70b-instruct":
  metadata:
    description: Llama 3.3 70B Instruct
    provider: Meta
    gpu_recommendation: Nvidia GPUs with at least 80GBx2 VRAM (e.g about 2 H100 GPUs).
  service_config:
    name: bentovllm-llama3.3-70b-instruct-service
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: meta-llama/Llama-3.3-70B-Instruct
    max_model_len: 2048
    tensor_parallel_size: 2
    max_num_seqs: 1024
    tool_call_parser: "pythonic"
    enable_auto_tool_choice: true
  build:
    exclude:
      - "original"
      - "consolidated*"
  generate_config:
    max_tokens: 1024
"pixtral-12b-2409":
  metadata:
    description: Pixtral 12B 2409
    provider: Mistral AI
    gpu_recommendation: an Nvidia GPU with at least 80GB VRAM (e.g about 1 H100 GPU).
  vision: true
  service_config:
    name: bentovllm-pixtral-12b-2409-service
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: mistralai/Pixtral-12B-2409
    tokenizer_mode: mistral
    config_format: mistral
    load_format: mistral
    limit_mm_per_prompt:
      image: 5
    max_model_len: 32768
    enable_prefix_caching: false
    max_num_seqs: 1024
    tool_call_parser: "mistral"
    enable_auto_tool_choice: true
  requirements:
    - mistral_common[opencv]
  generate_config:
    max_tokens: 4096
  build:
    exclude:
      - "model*"
"ministral-8b-instruct-2410":
  metadata:
    description: Ministral 8B Instruct 2410
    provider: Mistral AI
    gpu_recommendation: an Nvidia GPU with at least 24GB VRAM (e.g about 1 L4 GPU).
  service_config:
    name: bentovllm-ministral-8b-instruct-2410-service
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: mistralai/Ministral-8B-Instruct-2410
    tokenizer_mode: mistral
    config_format: mistral
    load_format: mistral
    max_model_len: 4096
    enable_prefix_caching: false
    tool_call_parser: "mistral"
    enable_auto_tool_choice: true
  build:
    exclude:
      - "model*"
  generate_config:
    max_tokens: 2048
  requirements:
    - mistral_common[opencv]
"mistral-small-3.1-24b-instruct-2503":
  vision: true
  metadata:
    description: Mistral Small 3.1 24B Instruct 2503 with Vision and Reasoning capabilities
    provider: Mistral AI
    gpu_recommendation: Nvidia GPUs with at least 80GBx2 VRAM (e.g about 2 H100 GPU).
  service_config:
    name: bentovllm-mistral-small-3.1-24b-instruct-2503-service
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: mistralai/Mistral-Small-3.1-24B-Instruct-2503
    tokenizer_mode: mistral
    config_format: mistral
    load_format: mistral
    max_model_len: 8192
    tensor_parallel_size: 2
    max_num_seqs: 1024
    limit_mm_per_prompt:
      image: 10
    enable_prefix_caching: false
    enable_auto_tool_choice: true
    tool_call_parser: "mistral"
  build:
    exclude:
      - "model*"
    post:
      - uv pip install --compile-bytecode vllm --pre --extra-index-url https://wheels.vllm.ai/nightly
  system_prompt: |
    You are Mistral Small 3.1, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.
    You power an AI assistant called Le Chat.
    Your knowledge base was last updated on 2023-10-01.
    The current date is {today}.

    When you're not sure about some information, you say that you don't have the information and don't make up anything.
    If the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. "What are some good restaurants around me?" => "Where are you?" or "When is the next flight to Tokyo" => "Where do you travel from?").
    You are always very attentive to dates, in particular you try to resolve dates (e.g. "yesterday" is {yesterday}) and when asked about information at specific dates, you discard information that is at another date.
    You follow these instructions in all languages, and always respond to the user in the language they use or request.
    Next sections describe the capabilities that you have.

    # WEB BROWSING INSTRUCTIONS

    You cannot perform any web search or access internet to open URLs, links etc. If it seems like the user is expecting you to do so, you clarify the situation and ask the user to copy paste the text directly in the chat.

    # MULTI-MODAL INSTRUCTIONS

    You have the ability to read images, but you cannot generate images. You also cannot transcribe audio files or videos.
    You cannot read nor transcribe audio files or videos.
  generate_config:
    max_tokens: 4096
  requirements:
    - mistral_common[opencv]
"mistral-small-24b-instruct-2501":
  metadata:
    description: Mistral Small 24B Instruct 2501
    provider: Mistral AI
    gpu_recommendation: an Nvidia GPU with at least 80GB VRAM (e.g about 1 H100 GPU).
  service_config:
    name: bentovllm-mistral-small-24b-instruct-2501-service
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: mistralai/Mistral-Small-24B-Instruct-2501
    tokenizer_mode: mistral
    config_format: mistral
    load_format: mistral
    max_model_len: 4096
    enable_prefix_caching: false
    max_num_seqs: 1024
    tool_call_parser: "mistral"
    enable_auto_tool_choice: true
  build:
    exclude:
      - "model*"
  generate_config:
    max_tokens: 2048
  requirements:
    - mistral_common[opencv]
"phi4-14b":
  metadata:
    description: Phi 4 14B
    provider: Microsoft
    gpu_recommendation: an Nvidia GPU with at least 80GB VRAM (e.g about 1 H100 GPU).
  service_config:
    name: bentovllm-phi4-14b-service
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: microsoft/phi-4
    max_model_len: 8192
    max_num_seqs: 1024
    chat_template: "{% if messages[0]['role'] == 'system' %}\n    {% set offset = 1 %}\n{% else %}\n    {% set offset = 0 %}\n{% endif %}\n\n{% for message in messages %}\n    {% if (message['role'] == 'user') != (loop.index0 % 2 == offset) %}\n        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n    {% endif %}\n\n    {{ '<|' + message['role'] + '|>\\n' + message['content'].strip() + '<|end|>' + '\\n' }}\n\n    {% if loop.last and message['role'] == 'user' and add_generation_prompt %}\n        {{ '<|assistant|>\\n' }}\n    {% endif %}\n{% endfor %}\n"
  generate_config:
    max_tokens: 4096
"qwen2.5-7b-instruct":
  metadata:
    description: Qwen 2.5 7B Instruct
    provider: Alibaba
    gpu_recommendation: an Nvidia GPU with at least 24GB VRAM (e.g about 1 L4 GPU).
  service_config:
    name: bentovllm-qwen2.5-7b-instruct-service
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
  engine_config:
    model: Qwen/Qwen2.5-7B-Instruct
    max_model_len: 2048
    enable_auto_tool_choice: true
    tool_call_parser: "llama3_json"
  generate_config:
    max_tokens: 1024
"qwen2.5-72b-instruct":
  metadata:
    description: Qwen 2.5 72B Instruct
    provider: Alibaba
    gpu_recommendation: Nvidia GPUs with at least 80GBx2 VRAM (e.g about 2 H100 GPUs).
  service_config:
    name: bentovllm-qwen2.5-72b-instruct-service
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: Qwen/Qwen2.5-72B-Instruct
    max_model_len: 2048
    tensor_parallel_size: 2
    max_num_seqs: 1024
    enable_auto_tool_choice: true
    tool_call_parser: "llama3_json"
  generate_config:
    max_tokens: 1024
"qwq-32b":
  reasoning: true
  metadata:
    description: QWQ 32B
    provider: Alibaba
    gpu_recommendation: an Nvidia GPU with at least 80GB VRAM (e.g about 1 H100 GPU).
  service_config:
    name: bentovllm-qwq-32b-service
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
      - name: UV_NO_BUILD_ISOLATION
        value: 1
  engine_config:
    model: Qwen/QwQ-32B
    max_model_len: 4096
    tensor_parallel_size: 1
    enable_reasoning: true
    reasoning_parser: "deepseek_r1"
    max_num_seqs: 1024
    enable_auto_tool_choice: true
    tool_call_parser: "llama3_json"
  generate_config:
    max_tokens: 2048
