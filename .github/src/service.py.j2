from __future__ import annotations

import logging, os, contextlib, typing
import bentoml, fastapi

logger = logging.getLogger(__name__)

MAX_TOKENS={{generate_config['max_tokens']|default(engine_config['max_model_len'])}}
ENGINE_CONFIG={{engine_config}}
{%- set ignore_patterns = exclude | map('tojson') | join(", ")%}

openai_api_app = fastapi.FastAPI()

@bentoml.asgi_app(openai_api_app, path='/v1')
@bentoml.service(
  {%- for key, value in service_config.items() -%}
  {%- if key == "name" -%}
  {{key}}="{{value}}",
  {%- elif key == "envs" and (value|length > 0) %}
  {{key}}=[
    {%- for value in service_config["envs"] %}
      {{value}},
    {%- endfor %}
  ],
  {%- else -%}
  {{key}}={{value}},
  {%- endif -%}
  {%- endfor %}
  labels={{labels}},
  image = bentoml.images.PythonImage(python_version='3.11', lock_python_packages=False)
        {%- if build['system_packages'] is defined %}
        {%- for item in build['system_packages'] -%}
        .system_packages("{{item}}")
        {%- endfor %}
        {%- endif %}
        {%- if build['pre'] is defined %}
        {%- for item in build['pre'] -%}
        .run("{{item}}")
        {%- endfor %}
        {%- endif %}
        .requirements_file('requirements.txt')
        {%- if build['post'] | length > 0 %}
        {%- for item in build['post'] -%}
        .run("{{item}}")
        {%- endfor %}
        {%- endif %}
)
class VLLM:
  model_id = "{{model_id}}"
  model = bentoml.models.HuggingFaceModel(model_id, exclude=[{{ignore_patterns}}])

  def __init__(self):
    {% if (reasoning | lower == "true") or (vision | lower == "true") or (embeddings | lower == "true") %}
    from openai import AsyncOpenAI
    self.openai = AsyncOpenAI(base_url="http://127.0.0.1:3000/v1", api_key="dummy")
    {%- endif %}
    self.exit_stack = contextlib.AsyncExitStack()

  @bentoml.on_startup
  async def init_engine(self) -> None:
    import vllm.entrypoints.openai.api_server as vllm_api_server

    from vllm.utils import FlexibleArgumentParser
    from vllm.entrypoints.openai.cli_args import make_arg_parser

    args = make_arg_parser(FlexibleArgumentParser()).parse_args([])
    args.model = self.model
    args.disable_log_requests = True
    args.max_log_len = 1000
    args.served_model_name = [self.model_id]
    args.request_logger = None
    args.disable_log_stats = True
    args.use_tqdm_on_load = False
    for key, value in ENGINE_CONFIG.items(): setattr(args, key, value)
    {%- if server_config | length > 0 %}
    {% for key, value in server_config.items() -%}
    {%- if key == "chat_template" %}
    args.{{key}} = """{{value}}"""
    {% elif value is string -%}
    args.{{key}} = "{{value}}"
    {% else -%}
    args.{{key}} = {{value}}
    {% endif -%}
    {%- endfor %}
    {%- endif %}

    router = fastapi.APIRouter(lifespan=vllm_api_server.lifespan)
    OPENAI_ENDPOINTS = [
      ["/chat/completions", vllm_api_server.create_chat_completion, ["POST"]],
      {%- if embeddings | lower == "true" %}
      ["/embeddings", vllm_api_server.create_embedding, ["POST"]],
      {%- endif %}
      ["/models", vllm_api_server.show_available_models, ["GET"]],
    ]

    for route, endpoint, methods in OPENAI_ENDPOINTS: router.add_api_route(path=route, endpoint=endpoint, methods=methods, include_in_schema=True)
    openai_api_app.include_router(router)

    self.engine = await self.exit_stack.enter_async_context(vllm_api_server.build_async_engine_client(args))
    self.model_config = await self.engine.get_model_config()
    self.tokenizer = await self.engine.get_tokenizer()
    await vllm_api_server.init_app_state(self.engine, self.model_config, openai_api_app.state, args)

  @bentoml.on_shutdown
  async def teardown_engine(self): await self.exit_stack.aclose()
