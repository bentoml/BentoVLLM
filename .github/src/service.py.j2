from __future__ import annotations

import {%- if vision | lower == "true" %} base64, io,{%- endif %} logging, contextlib{%- if vision | lower == "true" or reasoning | lower == "true" %}, traceback{%- endif %}, typing{%- if reasoning | lower != "true" %}, uuid{%- endif %}
import bentoml, pydantic, fastapi{%- if vision | lower == "true" %}, PIL.Image{%-endif %}, typing_extensions, annotated_types

logger = logging.getLogger(__name__)

{%- if system_prompt is not none %}
SYSTEM_PROMPT="""{{system_prompt -}}"""
{%- endif %}
{%- set ignore_patterns = exclude | map('tojson') | join(", ")%}

if typing.TYPE_CHECKING:
  from vllm.engine.arg_utils import EngineArgs
  class Args(EngineArgs, pydantic.BaseModel): pass
else:
  Args = pydantic.BaseModel

class BentoArgs(Args):
  bentovllm_model_id: str = "{{model_id}}"
  bentovllm_max_tokens: int = {{generate_config['max_tokens']|default(engine_config['max_model_len'])}}

  disable_log_requests: bool = True
  max_log_len: int = 1000
  request_logger: typing.Any = None
  disable_log_stats: bool = True
  use_tqdm_on_load: bool = False
  task: str = "{{task}}"

  {%- set all_configs = engine_config.copy() %}
  {%- for key, value in all_configs.items() %}
  {%- if key == "chat_template" %}
  {{key}}: str = """{{value}}"""
  {%- elif value is string %}
  {{key}}: str = "{{value}}"
  {%- elif value is integer %}
  {{key}}: int = {{value}}
  {%- elif value is boolean %}
  {{key}}: bool = {{value}}
  {%- elif value is mapping %}
  {{key}}: dict[str, typing.Any] = {{value}}
  {%- else %}
  {{key}}: typing.Any = {{value}}
  {%- endif %}
  {%- endfor %}

  @pydantic.model_serializer
  def serialize_model(self) -> dict[str, typing.Any]: return {k: getattr(self, k) for k in self.__class__.model_fields if not k.startswith("bentovllm_")}

bento_args = bentoml.use_arguments(BentoArgs)
openai_api_app = fastapi.FastAPI()

@bentoml.asgi_app(openai_api_app, path='/v1')
@bentoml.service(
  {%- for key, value in service_config.items() -%}
  {%- if key == "name" -%}
  {{key}}="{{value}}",
  {%- elif key == "envs" and (value|length > 0) %}
  {{key}}=[
    {%- for value in service_config["envs"] %}
      {{value}},
    {%- endfor %}
  ],
  {%- elif key == "resources" %}
  {{key}}={'gpu': bento_args.tensor_parallel_size, 'gpu_type': "{{value['gpu_type']}}"},
  {%- else -%}
  {{key}}={{value}},
  {%- endif -%}
  {%- endfor %}
  labels={{labels}},
  image = bentoml.images.Image(python_version='3.11', lock_python_packages={{not nightly}})
        {%- if build['system_packages'] | length > 0 %}
        {%- for item in build['system_packages'] -%}
        .system_packages("{{item}}")
        {%- endfor %}
        {%- endif %}
        {%- if build['pre'] is defined %}
        {%- for item in build['pre'] -%}
        .run("{{item}}")
        {%- endfor %}
        {%- endif %}
        .requirements_file('requirements.txt')
        {%- if build['post'] | length > 0 %}
        {%- for item in build['post'] -%}
        .run("{{item}}")
        {%- endfor %}
        {%- endif %}
)
class VLLM:
  model = bentoml.models.HuggingFaceModel(bento_args.bentovllm_model_id, exclude=[{{ignore_patterns}}])

  def __init__(self):
    {% if (reasoning | lower == "true") or (vision | lower == "true") or (embeddings | lower == "true") %}
    from openai import AsyncOpenAI
    self.openai = AsyncOpenAI(base_url="http://127.0.0.1:3000/v1", api_key="dummy")
    {%- endif %}
    self.exit_stack = contextlib.AsyncExitStack()

  @bentoml.on_startup
  async def init_engine(self) -> None:
    import vllm.entrypoints.openai.api_server as vllm_api_server

    from vllm.utils import FlexibleArgumentParser
    from vllm.entrypoints.openai.cli_args import make_arg_parser

    args = make_arg_parser(FlexibleArgumentParser()).parse_args([])
    args.model = self.model
    args.served_model_name = [bento_args.bentovllm_model_id]
    for key, value in bento_args.model_dump().items(): setattr(args, key, value)

    router = fastapi.APIRouter(lifespan=vllm_api_server.lifespan)
    OPENAI_ENDPOINTS = [
      ["/chat/completions", vllm_api_server.create_chat_completion, ["POST"]],
      {%- if embeddings | lower == "true" %}
      ["/embeddings", vllm_api_server.create_embedding, ["POST"]],
      {%- endif %}
      ["/models", vllm_api_server.show_available_models, ["GET"]],
    ]

    for route, endpoint, methods in OPENAI_ENDPOINTS: router.add_api_route(path=route, endpoint=endpoint, methods=methods, include_in_schema=True)
    openai_api_app.include_router(router)

    self.engine = await self.exit_stack.enter_async_context(vllm_api_server.build_async_engine_client(args))
    self.tokenizer = await self.engine.get_tokenizer()
    vllm_config = await self.engine.get_vllm_config()
    self.model_config = await self.engine.get_model_config()
    await vllm_api_server.init_app_state(self.engine, vllm_config, openai_api_app.state, args)

  @bentoml.on_shutdown
  async def teardown_engine(self): await self.exit_stack.aclose()

  @bentoml.api
  async def generate(
    self,
    {%- if prompt is not none %}
    prompt: str = """{{prompt -}}""",
    {%- else %}
    prompt: str = "Who are you? Please respond in pirate speak!",
    {%- endif %}
    {%- if system_prompt is not none %}
    system_prompt: typing.Optional[str]=SYSTEM_PROMPT,
    {%- endif %}
    max_tokens: typing_extensions.Annotated[int, annotated_types.Ge(128), annotated_types.Le(bento_args.bentovllm_max_tokens)] = bento_args.bentovllm_max_tokens,
    {%- if reasoning | lower == "true" %}
    show_reasoning: bool = True,
    {%- endif %}
  ) -> typing.AsyncGenerator[str, None]:
    {%- if reasoning | lower != "true" %}
    from vllm import SamplingParams, TokensPrompt
    {%- if all_configs['tool_call_parser'] == "mistral" %}
    from vllm.entrypoints.chat_utils import apply_mistral_chat_template
    {%- else %}
    from vllm.entrypoints.chat_utils import parse_chat_messages, apply_hf_chat_template
    {%- endif %}
    {%- else %}
    from vllm.entrypoints.openai.protocol import DeltaMessage
    {%- endif %}

    {%- if system_prompt is not none %}
    if system_prompt is None: system_prompt = SYSTEM_PROMPT
    {%- endif %}

    messages = [
      {%- if system_prompt is not none %}
      {"role": "system", "content": system_prompt},
      {%- endif %}
      {"role": "user", "content": [{"type": "text", "text": prompt}]}
    ]

    {%- if reasoning | lower == "true" %}
    try:
      completion = await self.openai.chat.completions.create(model=bento_args.bentovllm_model_id, messages=messages, stream=True, max_tokens=max_tokens)
      async for chunk in completion:
        delta_choice = typing.cast(DeltaMessage, chunk.choices[0].delta)
        if hasattr(delta_choice, 'reasoning_content') and show_reasoning: yield delta_choice.reasoning_content or ""
        else: yield delta_choice.content or ""
    except Exception:
      logger.error(traceback.format_exc())
      yield "Internal error found. Check server logs for more information"
      return
    {%- else %}

    params = SamplingParams(max_tokens=max_tokens)

    {%- if all_configs['tool_call_parser'] == "mistral" %}
    prompt = TokensPrompt(prompt_token_ids=apply_mistral_chat_template(self.tokenizer, messages=messages))
    {%- else %}
    conversation, _ = parse_chat_messages(messages, self.model_config, self.tokenizer, content_format="string")
    prompt = TokensPrompt(prompt_token_ids=apply_hf_chat_template(self.tokenizer, conversation=conversation, tools=None, add_generation_prompt=True, continue_final_message=False, chat_template=None, tokenize=True))
    {%- endif %}

    stream = self.engine.generate(request_id=uuid.uuid4().hex, prompt=prompt, sampling_params=params)

    cursor = 0
    async for request_output in stream:
      text = request_output.outputs[0].text
      yield text[cursor:]
      cursor = len(text)
    {%- endif %}

  {%- if vision | lower == "true" %}
  @bentoml.api
  async def sights(
    self,
    prompt: str = "Describe the content of the picture",
    {%- if system_prompt is not none %}
    system_prompt: typing.Optional[str]=SYSTEM_PROMPT,
    {%- endif %}
    image: typing.Optional["PIL.Image.Image"] = None,
    max_tokens: typing_extensions.Annotated[int, annotated_types.Ge(128), annotated_types.Le(bento_args.bentovllm_max_tokens)] = bento_args.bentovllm_max_tokens
  ) -> typing.AsyncGenerator[str, None]:
    if image:
      buffered = io.BytesIO()
      image.save(buffered, format="PNG")
      img_str = base64.b64encode(buffered.getvalue()).decode()
      buffered.close()
      image_url = f"data:image/png;base64,{img_str}"
      content = [dict(type="image_url", image_url=dict(url=image_url)), dict(type="text", text=prompt)]
    else:
      content = [dict(type="text", text=prompt)]

    {%- if system_prompt is not none %}
    if system_prompt is None: system_prompt = SYSTEM_PROMPT
    {%- endif %}
    messages = [
      {%- if system_prompt is not none %}
      {"role": "system", "content": system_prompt},
      {%- endif %}
      {"role": "user", "content": content}
    ]

    try:
      completion = await self.openai.chat.completions.create(model=bento_args.bentovllm_model_id, messages=messages, stream=True, max_tokens=max_tokens)
      async for chunk in completion: yield chunk.choices[0].delta.content or ""
    except Exception:
      logger.error(traceback.format_exc())
      yield "Internal error found. Check server logs for more information"
      return
  {%- endif %}

  {%- if embeddings | lower == "true" %}
  @bentoml.api
  async def embedding(self, prompt: str = "Life is a meaning and a construct of self."):
      try:
          return await self.client.embeddings.create(input=[prompt], model=bento_args.bentovllm_model_id)
      except Exception:
          logger.error(traceback.format_exc())
          return "Internal error found. Check server logs for more information"
  {%- endif %}
