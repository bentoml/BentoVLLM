from __future__ import annotations

import logging, contextlib, typing
import bentoml, pydantic, fastapi, typing_extensions, annotated_types

logger = logging.getLogger(__name__)

{%- if system_prompt is not none %}
SYSTEM_PROMPT="""{{system_prompt -}}"""
{%- endif %}
{%- set ignore_patterns = exclude | map('tojson') | join(", ")%}

if typing.TYPE_CHECKING:
  from vllm.engine.arg_utils import EngineArgs
  class Args(EngineArgs, pydantic.BaseModel): pass
else:
  Args = pydantic.BaseModel

class BentoArgs(Args):
  bentovllm_model_id: str = "{{model_id}}"
  bentovllm_max_tokens: int = {{generate_config['max_tokens']|default(engine_config['max_model_len'])}}

  disable_log_requests: bool = True
  max_log_len: int = 1000
  request_logger: typing.Any = None
  disable_log_stats: bool = True
  use_tqdm_on_load: bool = False
  task: str = "{{task}}"

  {%- set all_configs = engine_config.copy() %}
  {%- for key, value in all_configs.items() %}
  {%- if key == "chat_template" %}
  {{key}}: str = """{{value}}"""
  {%- elif value is string %}
  {{key}}: str = "{{value}}"
  {%- elif value is integer %}
  {{key}}: int = {{value}}
  {%- elif value is boolean %}
  {{key}}: bool = {{value}}
  {%- elif value is mapping %}
  {{key}}: dict[str, typing.Any] = {{value}}
  {%- else %}
  {{key}}: typing.Any = {{value}}
  {%- endif %}
  {%- endfor %}

  @pydantic.model_serializer
  def serialize_model(self) -> dict[str, typing.Any]: return {k: getattr(self, k) for k in self.__class__.model_fields if not k.startswith("bentovllm_")}

bento_args = bentoml.use_arguments(BentoArgs)
openai_api_app = fastapi.FastAPI()

@bentoml.asgi_app(openai_api_app, path='/v1')
@bentoml.service(
  {%- for key, value in service_config.items() -%}
  {%- if key == "name" -%}
  {{key}}="{{value}}",
  {%- elif key == "envs" and (value|length > 0) %}
  {{key}}=[
    {%- for value in service_config["envs"] %}
      {{value}},
    {%- endfor %}
  ],
  {%- elif key == "resources" %}
  {{key}}={'gpu': bento_args.tensor_parallel_size, 'gpu_type': "{{value['gpu_type']}}"},
  {%- else -%}
  {{key}}={{value}},
  {%- endif -%}
  {%- endfor %}
  labels={{labels}},
  image = bentoml.images.Image(python_version='3.11')
        {%- if build['system_packages'] is defined %}
        {%- for item in build['system_packages'] -%}
        .system_packages("{{item}}")
        {%- endfor %}
        {%- endif %}
        {%- if build['pre'] is defined %}
        {%- for item in build['pre'] -%}
        .run("{{item}}")
        {%- endfor %}
        {%- endif %}
        .requirements_file('requirements.txt')
        {%- if build['post'] | length > 0 %}
        {%- for item in build['post'] -%}
        .run("{{item}}")
        {%- endfor %}
        {%- endif %}
)
class VLLM:
  model = bentoml.models.HuggingFaceModel(bento_args.bentovllm_model_id, exclude=[{{ignore_patterns}}])

  def __init__(self): self.exit_stack = contextlib.AsyncExitStack()

  @bentoml.on_startup
  async def init_engine(self) -> None:
    import vllm.entrypoints.openai.api_server as vllm_api_server

    from vllm.utils import FlexibleArgumentParser
    from vllm.entrypoints.openai.cli_args import make_arg_parser

    args = make_arg_parser(FlexibleArgumentParser()).parse_args([])
    args.model = self.model
    args.served_model_name = [bento_args.bentovllm_model_id]
    for key, value in bento_args.model_dump().items(): setattr(args, key, value)

    router = fastapi.APIRouter(lifespan=vllm_api_server.lifespan)
    OPENAI_ENDPOINTS = [
      ["/chat/completions", vllm_api_server.create_chat_completion, ["POST"]],
      {%- if embeddings | lower == "true" %}
      ["/embeddings", vllm_api_server.create_embedding, ["POST"]],
      {%- endif %}
      ["/models", vllm_api_server.show_available_models, ["GET"]],
    ]

    for route, endpoint, methods in OPENAI_ENDPOINTS: router.add_api_route(path=route, endpoint=endpoint, methods=methods, include_in_schema=True)
    openai_api_app.include_router(router)

    engine = await self.exit_stack.enter_async_context(vllm_api_server.build_async_engine_client(args))
    vllm_config = await engine.get_vllm_config()
    await vllm_api_server.init_app_state(engine, vllm_config, openai_api_app.state, args)

  @bentoml.on_shutdown
  async def teardown_engine(self): await self.exit_stack.aclose()
