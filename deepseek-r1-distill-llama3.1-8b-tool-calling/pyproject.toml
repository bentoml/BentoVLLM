[project]
name = "bentovllm-deepseek-r1-distill-llama3.1-8b-tool-calling-service"
description = "Self-host deepseek-ai/DeepSeek-R1-Distill-Llama-8B with vLLM and BentoML"
readme = "README.md"
requires-python = ">=3.10"
license = { text = "Apache-2.0" }
authors = [{ name = "BentoML Team", email = "contact@bentoml.com" }]
dependencies = [
  "bentoml>=1.3.20",
  "vllm==0.7.1",
  "kantoku>=0.18.1",
  "openai>=1.61.0",
  "pyyaml",
  "Pillow",
  "flashinfer-python",
]
version = "0.0.0"

[project.urls]
Website = "https://bentoml.com"
Documentation = "https://docs.bentoml.com"
GitHub = "https://github.com/bentoml/BentoVLLM"
Twitter = "https://twitter.com/bentomlai"
Tracker = "https://github.com/bentoml/BentoVLLM"

[tool.bentoml.build]
service = "service:VLLM"
include = ["LICENCE", "*.py", "*.toml", "*.md"]

[tool.bentoml.build.labels]
owner = "bentoml-team"
stage = "prebuilt"
registry = "huggingface"