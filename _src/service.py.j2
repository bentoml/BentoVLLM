from __future__ import annotations

import base64, io, logging, traceback, typing, argparse, asyncio, os
import bentoml, fastapi, PIL.Image, typing_extensions, annotated_types

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

ENGINE_CONFIG = {{engine_config}}
MAX_TOKENS = {{generate_config['max_tokens']|default(engine_config['max_model_len'])}}
{%- set reqs = requirements|default([]) %}

openai_api_app = fastapi.FastAPI()

@bentoml.asgi_app(openai_api_app, path='/v1')
@bentoml.service(
  {%- for key, value in service_config.items() -%}
  {%- if key == "name" -%}
  {{key}}="{{value}}",
  {%- else -%}
  {{key}}={{value}},
  {%- endif -%}
  {%- endfor %}
  labels={{labels}},
  image = bentoml.images.PythonImage(python_version='3.11', lock_python_packages={{lock_python_packages}})
        {%- if build['pre'] is defined %}
        {%- for item in build['pre'] -%}
        .run("{{item}}")
        {%- endfor %}
        {%- endif %}
        .python_packages('-i https://flashinfer.ai/whl/cu124/torch2.5/')
        .requirements_file('requirements.txt')
        {%- if build['post'] is defined %}
        {%- for item in build['post'] -%}
        .run("{{item}}")
        {%- endfor %}
        {%- endif %}
)
class VLLM:
  model_id = ENGINE_CONFIG["model"]
  model = bentoml.models.HuggingFaceModel(model_id, exclude=[{{exclude | map('tojson') | join(", ")}}])

  def __init__(self) -> None:
    from vllm import AsyncEngineArgs, AsyncLLMEngine
    import vllm.entrypoints.openai.api_server as vllm_api_server

    OPENAI_ENDPOINTS = [
      ["/chat/completions", vllm_api_server.create_chat_completion, ["POST"]],
      {%- if metadata['embeddings']|default(false) %}
      ["/embeddings", vllm_api_server.create_embedding, ["POST"]],
      {%- endif %}
      ["/models", vllm_api_server.show_available_models, ["GET"]],
    ]
    for route, endpoint, methods in OPENAI_ENDPOINTS: openai_api_app.add_api_route(path=route, endpoint=endpoint, methods=methods, include_in_schema=True)

    ENGINE_ARGS = AsyncEngineArgs(**dict(ENGINE_CONFIG, model=self.model))
    engine = AsyncLLMEngine.from_engine_args(ENGINE_ARGS)
    model_config = engine.engine.get_model_config()

    args = argparse.Namespace()
    args.model = self.model
    args.disable_log_requests = True
    args.max_log_len = 1000
    args.response_role = "assistant"
    args.served_model_name = [self.model_id]
    args.chat_template = None
    args.chat_template_content_format = "auto"
    args.lora_modules = None
    args.prompt_adapters = None
    args.request_logger = None
    args.disable_log_stats = True
    args.return_tokens_as_token_ids = False
    args.enable_tool_call_parser = False
    args.enable_auto_tool_choice = False
    args.tool_call_parser = None
    args.enable_prompt_tokens_details = False
    args.enable_reasoning = False
    args.reasoning_parser = None
    {%- if server_config | length > 0 %}
    {% for key, value in server_config.items() -%}
    {%- if key == "chat_template" %}
    args.{{key}} = """{{value}}"""
    {% elif value is string -%}
    args.{{key}} = "{{value}}"
    {% else -%}
    args.{{key}} = {{value}}
    {% endif -%}
    {%- endfor %}
    {%- endif %}

    asyncio.create_task(vllm_api_server.init_app_state(engine, model_config, openai_api_app.state, args))

  @bentoml.api
  async def generate(
    self,
    prompt: str = "Who are you? Please respond in pirate speak!",
    max_tokens: typing_extensions.Annotated[int, annotated_types.Ge(128), annotated_types.Le(MAX_TOKENS)] = MAX_TOKENS
  ) -> typing.AsyncGenerator[str, None]:
    from openai import AsyncOpenAI

    client = AsyncOpenAI(base_url="http://127.0.0.1:3000/v1", api_key="dummy")
    try:
      completion = await client.chat.completions.create(model=self.model_id, messages=[dict(role="user", content=[dict(type="text", text=prompt)])], stream=True, max_tokens=max_tokens)
      async for chunk in completion: yield chunk.choices[0].delta.content or ""
    except Exception:
      logger.error(traceback.format_exc())
      yield "Internal error found. Check server logs for more information"
      return

  {%- if vision | lower == "true" %}
  @bentoml.api
  async def sights(
    self, prompt: str = "Describe the content of the picture", image: typing.Optional["PIL.Image.Image"] = None,
    max_tokens: typing_extensions.Annotated[int, annotated_types.Ge(128), annotated_types.Le(MAX_TOKENS)] = MAX_TOKENS
  ) -> typing.AsyncGenerator[str, None]:
    from openai import AsyncOpenAI

    client = AsyncOpenAI(base_url="http://127.0.0.1:3000/v1", api_key="dummy")
    if image:
      buffered = io.BytesIO()
      image.save(buffered, format="PNG")
      img_str = base64.b64encode(buffered.getvalue()).decode()
      buffered.close()
      image_url = f"data:image/png;base64,{img_str}"
      content = [dict(type="image_url", image_url=dict(url=image_url)), dict(type="text", text=prompt)]
    else:
      content = [dict(type="text", text=prompt)]

    try:
      completion = await client.chat.completions.create(model=self.model_id, messages=[dict(role="user", content=content)], stream=True, max_tokens=max_tokens)
      async for chunk in completion: yield chunk.choices[0].delta.content or ""
    except Exception:
      logger.error(traceback.format_exc())
      yield "Internal error found. Check server logs for more information"
      return
  {%- endif %}
