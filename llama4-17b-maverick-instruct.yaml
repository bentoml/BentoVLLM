args:
  name: llama4-17b-maverick-instruct
  gpu_type: nvidia-h200-141gb
  tp: 8
  sharded: true
  metadata:
    vision: true
    description: Llama 4 Maverick 17B-128E Instruct
    provider: Meta
    gpu_recommendation: Nvidia GPUs with at least 141GBx8 VRAM (e.g about 8 H200 GPUs).
  model_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
  tool_parser: pythonic
  cli_args:
    - '--max-model-len'
    - '1000000'
  envs:
    - name: HF_TOKEN
    - name: VLLM_DISABLE_COMPILE_CACHE
      value: "1"
  exclude:
      - "original"
      - "consolidated*"
      - "*.pth"
      - "*.pt"
  hf_generation_config:
    temperature: 0.6
    top_p: 0.9
