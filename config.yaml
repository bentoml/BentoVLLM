"deepseek-v3-671b":
  service_config:
    name: deepseek-v3
    traffic:
      timeout: 300
    resources:
      gpu: 16
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-V3
    max_model_len: 2048
    tensor_parallel_size: 16
"deepseek-r1-671b":
  service_config:
    name: deepseek-r1
    traffic:
      timeout: 300
    resources:
      gpu: 16
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-R1
    tensor_parallel_size: 16
    trust_remote_code: True
  server_config:
    enable_reasoning: True
    reasoning_parser: deepseek_r1
"deepseek-r1-distill-llama3.3-70b":
  service_config:
    name: deepseek-r1-distill
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-R1-Distill-Llama-70B
    tensor_parallel_size: 2
"deepseek-r1-distill-qwen2.5-32b":
  service_config:
    name: deepseek-r1-distill
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    traffic:
      timeout: 300
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
"deepseek-r1-distill-qwen2.5-14b":
  service_config:
    name: deepseek-r1-distill
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-R1-Distill-Qwen-14B
"deepseek-r1-distill-qwen2.5-7b-math":
  service_config:
    name: deepseek-r1-distill
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
"deepseek-r1-distill-llama3.1-8b":
  service_config:
    name: deepseek-r1-distill
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-R1-Distill-Llama-8B
"deepseek-r1-distill-llama3.1-8b-tool-calling":
  service_config:
    name: deepseek-r1-distill
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: deepseek-ai/DeepSeek-R1-Distill-Llama-8B
  server_config:
    enable_auto_tool_choice: True
    enable_tool_call_parser: True
    tool_call_parser: "llama3_json"
"gemma2-2b-instruct":
  service_config:
    name: gemma2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: google/gemma-2-2b-it
    max_model_len: 2048
    dtype: half
"gemma2-9b-instruct":
  service_config:
    name: gemma2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    envs:
      - name: VLLM_ATTENTION_BACKEND
        value: FLASHINFER
      - name: HF_TOKEN
  engine_config:
    model: google/gemma-2-9b-it
    max_model_len: 2048
    dtype: half
"gemma2-27b-instruct":
  service_config:
    name: gemma2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    envs:
      - name: VLLM_ATTENTION_BACKEND
        value: FLASHINFER
      - name: HF_TOKEN
  engine_config:
    model: google/gemma-2-27b-it
    max_model_len: 2048
    dtype: half
"jamba1.5-mini":
  service_config:
    name: jamba1.5
    traffic:
      timeout: 300
    resources:
      gpu: 4
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: ai21labs/AI21-Jamba-1.5-Mini
    max_model_len: 2048
    dtype: half
    tensor_parallel_size: 4
"llama3.1-8b-instruct":
  service_config:
    name: llama3.1
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-tesla-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: meta-llama/Meta-Llama-3.1-8B-Instruct
    max_model_len: 2048
    dtype: half
"llama3.2-1b-instruct":
  service_config:
    name: llama3.2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: meta-llama/Llama-3.2-1B-Instruct
    max_model_len: 16384
  server_config:
    tool_call_parser: "pythonic"
"llama3.2-3b-instruct":
  service_config:
    name: llama3.2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: meta-llama/Llama-3.2-3B-Instruct
    max_model_len: 16384
  server_config:
    tool_call_parser: "pythonic"
"llama3.2-11b-vision-instruct":
  vision: true
  service_config:
    name: llama3.2
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: meta-llama/Llama-3.2-11B-Vision-Instruct
    enforce_eager: true
    limit_mm_per_prompt:
      image: 1
    max_model_len: 16384
    max_num_seqs: 16
"llama3.2-90b-vision-instruct":
  vision: true
  service_config:
    name: llama3.2
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: meta-llama/Llama-3.2-90B-Vision-Instruct
    enforce_eager: true
    limit_mm_per_prompt:
      image: 1
    max_model_len: 16384
    max_num_seqs: 16
"llama3.3-70b-instruct":
  service_config:
    name: llama3.3
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: meta-llama/Llama-3.3-70B-Instruct
    max_model_len: 2048
    tensor_parallel_size: 2
"pixtral-12b-2409":
  vision: true
  service_config:
    name: pixtral
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: mistral-community/pixtral-12b-240910
    tokenizer_mode: mistral
    enable_prefix_caching: true
    enable_chunked_prefill: false
    limit_mm_per_prompt:
      image: 1
    max_model_len: 16384
  requirements:
    - mistral_common[opencv]
"mixtral-8x7b-v0.1":
  service_config:
    name: mixtral
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: mistralai/Mixtral-8x7B-Instruct-v0.1
    max_model_len: 2048
    tensor_parallel_size: 2
    tokenizer_mode: mistral
"ministral-8b-instruct-2410":
  service_config:
    name: mistral-mini
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    envs:
      - name: HF_TOKEN
  engine_config:
    model: mistralai/Ministral-8B-Instruct-2410
    dtype: half
    tokenizer_mode: mistral
"mistral-small-24b-instruct-2501":
  service_config:
    name: mistral-small
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: mistralai/Mistral-Small-24B-Instruct-2501
    tokenizer_mode: mistral
"mistral-large-123b-instruct":
  service_config:
    name: mistral-large
    traffic:
      timeout: 300
    resources:
      gpu: 4
      gpu_type: nvidia-a100-80gb
    envs:
      - name: HF_TOKEN
  engine_config:
    model: mistralai/Mistral-Large-Instruct-2407
    max_model_len: 4096
    tensor_parallel_size: 4
    tokenizer_mode: mistral
"phi4-14b":
  service_config:
    name: phi4
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: microsoft/phi-4
    max_model_len: 8192
"qwen2.5-7b-instruct":
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-l4
  engine_config:
    model: Qwen/Qwen2.5-7B-Instruct
    max_model_len: 2048
"qwen2.5-14b-instruct":
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: Qwen/Qwen2.5-14B-Instruct
    max_model_len: 2048
"qwen2.5-32b-instruct":
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: Qwen/Qwen2.5-32B-Instruct
    max_model_len: 2048
"qwen2.5-72b-instruct":
  service_config:
    name: qwen2.5
    traffic:
      timeout: 300
    resources:
      gpu: 2
      gpu_type: nvidia-a100-80gb
  engine_config:
    model: Qwen/Qwen2.5-72B-Instruct
    max_model_len: 2048
"qwen2.5-coder-7b-instruct":
  service_config:
    name: qwen2.5-coder
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    traffic:
      timeout: 300
  engine_config:
    model: Qwen/Qwen2.5-Coder-7B-Instruct
    max_model_len: 20480
  server_config:
    enable_auto_tool_choice: True
    enable_tool_call_parser: True
    tool_call_parser: "llama3_json"
"qwen2.5-coder-32b-instruct":
  project: vllm-chat
  service_config:
    name: qwen2.5-coder
    resources:
      gpu: 1
      gpu_type: nvidia-a100-80gb
    traffic:
      timeout: 300
  engine_config:
    model: Qwen/Qwen2.5-Coder-32B-Instruct
    max_model_len: 20480
  server_config:
    enable_auto_tool_choice: True
    enable_tool_call_parser: True
    tool_call_parser: "llama3_json"
"qwen2.5vl-3b-instruct":
  vision: true
  engine_config:
    max_model_len: 2048
    model: Qwen/Qwen2.5-VL-3B-Instruct
  service_config:
    name: qwen2.5vl
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    traffic:
      timeout: 300
  requirements:
    - qwen-vl-utils[decord]==0.0.8
"qwen2.5vl-7b-instruct":
  vision: true
  engine_config:
    max_model_len: 2048
    model: Qwen/Qwen2.5-VL-7B-Instruct
  service_config:
    name: qwen2.5vl
    resources:
      gpu: 1
      gpu_type: nvidia-l4
    traffic:
      timeout: 300
  requirements:
    - qwen-vl-utils[decord]==0.0.8

